# Network Telemetry Aggregation System

## Overview

This project implements a simplified network telemetry aggregation system inspired by NVIDIA UFM.  
The system simulates telemetry data generated by fabric switches, ingests this data periodically, and exposes it via a REST API for real-time querying.

The design focuses on:
- fast, non-blocking API responses
- predictable data freshness
- clear separation between ingestion and serving
- simplicity and correctness over premature optimization

---

## System Architecture

The system consists of **two independent services**:

1. **Telemetry Generator**  
   Simulates switches producing telemetry metrics and exposes them via an HTTP endpoint in CSV format.

2. **Metrics Server (Telemetry Aggregator)**  
   Periodically ingests telemetry data from the generator, stores the latest snapshot in memory, and serves it via REST APIs.



+---------------------+        HTTP (CSV)        +----------------------+
| Telemetry Generator |  -------------------->   |  Metrics Server      |
| (port 9001)         |                          |  (port 8080)         |
|                     |                          |                      |
| - Simulated metrics |                          | - Ingestion thread   |
| - CSV endpoint      |                          | - In-memory storage  |
+---------------------+                          | - REST API           |
                                                 +----------------------+


---

## Telemetry Generator

### Responsibilities
- Simulates a configurable number of switches.
- Generates realistic telemetry metrics:
  - bandwidth usage
  - latency (with occasional spikes)
  - packet errors
  - queue depth
  - utilization
  - link status (modeled as numeric: `1` = up, `0` = down)
- Updates metrics periodically with a fixed interval of 10 seconds.
- Exposes all telemetry data via a REST endpoint returning CSV.

** When link status is down (0), all other metrics for that switch are reported as zero.

### Endpoint
GET http://127.0.0.1:9001/counters

### Example response:
switch_id,bandwidth_mbps,latency_ms,packet_errors,link_status,tx_queue_depth,utilization_percent
sw1,8123,1.42,0,1,128,47.5
sw2,6400,6.8,2,1,512,63.1

### Configuration

The telemetry generator loads configuration from:
telemetry_generator/config/config.json

** Example configuration **:

{
"switches": { "count": 4 },
"metrics": [
"bandwidth_mbps",
"latency_ms",
"packet_errors",
"link_status",
"tx_queue_depth",
"utilization_percent"
]
}

If the configuration file is missing, sensible defaults are used.

---

## Metrics Server (Telemetry Aggregator)

### Responsibilities
- Periodically polls the telemetry generator at a fixed interval
- Parses CSV telemetry data
- Stores the latest snapshot only in memory
- Serves telemetry data via REST APIs
- Logs Request format, error code and API latency

### Data Storage
- Telemetry is stored as an immutable snapshot:
- Each ingestion cycle builds a new snapshot
- The snapshot reference is replaced atomically
- Readers always see a consistent view (old or new)
- No locks are required on the read path

This design allows:
- fast, non-blocking reads
- isolation between ingestion and serving


### REST API

Base URL:
http://127.0.0.1:8080/telemetry

#### GetMetric
Fetch the current value of a specific metric for a specific switch.

GET /telemetry/GetMetric?switch=sw1&metric=bandwidth_mbps

** Example response: **

{
"switch": "sw1",
"metric": "bandwidth_mbps",
"value": 8123,
"timestamp": 1766501919
}

#### ListMetrics
Fetch all current metrics for all switches.

GET /telemetry/ListMetrics

** Example response: **

{
"sw1": {
"bandwidth_mbps": 8123,
"latency_ms": 1.42,
"packet_errors": 0,
"link_status": 1,
"tx_queue_depth": 128,
"utilization_percent": 47.5,
"timestamp": 1766501919
}
}


### Performance and Observability

#### Performance Characteristics:
- API reads are non-blocking and O(1)
- Ingestion runs in a background thread
- Snapshot replacement avoids contention between readers and writers
- Suitable for many concurrent read requests within a single process

#### Observability
- API latency is measured per request using Flask request hooks
- Latency and errors are logged using Python’s standard logging module
- Logging level is configurable via configuration file

** Example log entry: **
INFO /telemetry/ListMetrics 200 0.19ms


### Error Handling
- Missing or invalid query parameters return HTTP 400
- Unknown switches or metrics return HTTP 404
- Ingestion failures are logged and do not block API serving
- The last known snapshot remains available if ingestion fails temporarily

### Testing
Basic unit tests are included for the Metrics Server API using pytest and Flask’s test client.

Tests cover:
- successful metric retrieval
- error handling for invalid input
- list metrics endpoint behavior

** Run tests: **

From project root run:

pytest

---

## Running the System

### Install dependencies

pip install -r requirements.txt

### Start the telemetry generator
The generator listens on 127.0.0.1:9001 by default. 
This can be overridden using the GENERATOR_HOST and GENERATOR_PORT environment variables.

python telemetry_generator/server.py

### Start the metrics server
The metrics server listens on 127.0.0.1:8080 by default. The bind address and port can be overridden using the METRICS_SERVER_HOST and METRICS_SERVER_PORT environment variables.

python -m metrics_server.api

---

## Scalability and Limitations

### Current Limitations
- Single-process execution
- In-memory storage only (no persistence)
- Poll-based ingestion limits freshness to polling interval
- No horizontal scaling or clustering
- Scalability Boundaries
- Memory and CPU of a single node
- CSV parsing cost grows linearly with number of switches and metrics
- Suitable for moderate scale and real-time snapshots

---

### Possible Improvements
- Externalize storage (e.g., Redis or distributed key-value store)
- Event-driven ingestion instead of polling
- Horizontal scaling with multiple stateless API instances
- Binary telemetry formats instead of CSV
- Historical data retention and aggregation

---

## Summary
This implementation demonstrates:
- clear separation of concerns
- efficient, non-blocking read path
- predictable ingestion behavior
- realistic telemetry simulation
- explicit reasoning about performance, data freshness, and scalability
- The system intentionally favors simplicity and clarity while remaining extensible toward production-grade architectures.